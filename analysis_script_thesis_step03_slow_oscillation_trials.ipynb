{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yasa\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pybv\n",
    "from scipy import signal\n",
    "from scipy import io\n",
    "import pymatreader\n",
    "from mne.io import Raw\n",
    "from mne import read_events\n",
    "import wonambi\n",
    "import h5py\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some preliminaires\n",
    "datapath = r'C:\\Users\\siann\\Data\\spindle_ppTMS\\EEG'\n",
    "subjects = 'sub-02'\n",
    "session = ['ses-exp_01', 'ses-exp_02', 'ses-exp_03']\n",
    "condition_peak = 'S155'\n",
    "condition_trough = 'S156'\n",
    "condition_rising = 'S157'\n",
    "condition_falling = 'S158'\n",
    "condition_sp_free = 'S159'\n",
    "sf = 5000\n",
    "os.chdir(datapath + '\\\\' + subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify all spindle events that are nested within slow oscillations vs. spindle events occuring in isolation. First, we we use YASA to automatically detect slow oscillations in our data. Then, for each TMS pulse given, we identify whether the spindle targeted on that trial was accompanied by a slow oscillation or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file C:\\Users\\siann\\Data\\spindle_ppTMS\\EEG/sub-02/data_tms_clean_unsegmented.fif...\n",
      "Isotrak not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Range : 0 ... 1844999 =      0.000 ...   369.000 secs\n",
      "Ready.\n",
      "Reading 0 ... 1844999  =      0.000 ...   369.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siann\\AppData\\Local\\Temp\\ipykernel_8988\\1134391566.py:2: RuntimeWarning: This filename (C:\\Users\\siann\\Data\\spindle_ppTMS\\EEG/sub-02/data_tms_clean_unsegmented.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw_data = mne.io.read_raw_fif(datapath + '/' + subjects + '/' + 'data_tms_clean_unsegmented.fif', preload = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 9 non-empty values\n",
      " bads: []\n",
      " ch_names: Fp1, Fp2, F3, F4, C3, C4, P3, P4, O1, O2, F7, F8, T7, T8, P7, ...\n",
      " chs: 70 misc\n",
      " custom_ref_applied: False\n",
      " file_id: 4 items (dict)\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 2500.0 Hz\n",
      " meas_date: 1970-01-01 00:00:00 UTC\n",
      " meas_id: 4 items (dict)\n",
      " nchan: 70\n",
      " projs: []\n",
      " sfreq: 5000.0 Hz\n",
      ">\n",
      "No data channels found. The highpass and lowpass values in the measurement info will not be updated.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 165001 samples (33.000 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    2.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      0       0       2]\n",
      " [  22500       0       5]\n",
      " [  45000       0       4]\n",
      " [  67500       0       3]\n",
      " [  90000       0       1]\n",
      " [ 112500       0       2]\n",
      " [ 135000       0       1]\n",
      " [ 157500       0       4]\n",
      " [ 180000       0       5]\n",
      " [ 202500       0       3]\n",
      " [ 225000       0       3]\n",
      " [ 247500       0       5]\n",
      " [ 270000       0       2]\n",
      " [ 292500       0       1]\n",
      " [ 315000       0       4]\n",
      " [ 337500       0       5]\n",
      " [ 360000       0       3]\n",
      " [ 382500       0       2]\n",
      " [ 405000       0       1]\n",
      " [ 427500       0       4]\n",
      " [ 450000       0       2]\n",
      " [ 472500       0       3]\n",
      " [ 495000       0       1]\n",
      " [ 517500       0       5]\n",
      " [ 540000       0       4]\n",
      " [ 562500       0       3]\n",
      " [ 585000       0       2]\n",
      " [ 607500       0       4]\n",
      " [ 630000       0       1]\n",
      " [ 652500       0       5]\n",
      " [ 675000       0       4]\n",
      " [ 697500       0       1]\n",
      " [ 720000       0       2]\n",
      " [ 742500       0       5]\n",
      " [ 765000       0       1]\n",
      " [ 787500       0       4]\n",
      " [ 810000       0       5]\n",
      " [ 832500       0       3]\n",
      " [ 855000       0       2]\n",
      " [ 877500       0       3]\n",
      " [ 900000       0       2]\n",
      " [ 922500       0       4]\n",
      " [ 945000       0       1]\n",
      " [ 967500       0       1]\n",
      " [ 990000       0       3]\n",
      " [1012500       0       3]\n",
      " [1035000       0       1]\n",
      " [1057500       0       4]\n",
      " [1080000       0       5]\n",
      " [1102500       0       2]\n",
      " [1125000       0       3]\n",
      " [1147500       0       5]\n",
      " [1170000       0       4]\n",
      " [1192500       0       1]\n",
      " [1215000       0       2]\n",
      " [1237500       0       4]\n",
      " [1260000       0       5]\n",
      " [1282500       0       1]\n",
      " [1305000       0       3]\n",
      " [1327500       0       2]\n",
      " [1350000       0       4]\n",
      " [1372500       0       2]\n",
      " [1395000       0       5]\n",
      " [1417500       0       1]\n",
      " [1440000       0       3]\n",
      " [1462500       0       3]\n",
      " [1485000       0       5]\n",
      " [1507500       0       2]\n",
      " [1530000       0       1]\n",
      " [1552500       0       4]\n",
      " [1575000       0       4]\n",
      " [1597500       0       5]\n",
      " [1620000       0       5]\n",
      " [1642500       0       3]\n",
      " [1665000       0       4]\n",
      " [1687500       0       2]\n",
      " [1710000       0       1]\n",
      " [1732500       0       4]\n",
      " [1755000       0       2]\n",
      " [1777500       0       5]\n",
      " [1800000       0       3]\n",
      " [1822500       0       1]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siann\\AppData\\Local\\Temp\\ipykernel_8988\\1134391566.py:7: RuntimeWarning: This filename (C:\\Users\\siann\\Data\\spindle_ppTMS\\EEG/sub-02/data_tms_clean_segmented.fif) does not conform to MNE naming conventions. All events files should end with .eve, -eve.fif, -eve.fif.gz, -eve.lst, -eve.txt, _eve.fif, _eve.fif.gz, _eve.lst, _eve.txt or -annot.fif\n",
      "  events = read_events(datapath + '/' + subjects + '/' + 'data_tms_clean_segmented.fif')\n"
     ]
    }
   ],
   "source": [
    "# read in the data cleaned of TMS artifacts as one continous dataset\n",
    "raw_data = mne.io.read_raw_fif(datapath + '/' + subjects + '/' + 'data_tms_clean_unsegmented.fif', preload = True)\n",
    "print(raw_data.info)\n",
    "raw_data.filter(0.1, 40, picks=raw_data.ch_names) # bp filter\n",
    "# and get trials based on stimulation times and conditions     \n",
    "events = read_events(datapath + '/' + subjects + '/' + 'data_tms_clean_segmented.fif')\n",
    "print(events)\n",
    "\n",
    "# rereference?\n",
    "# change semgentation of data to be -0.504 to -0.004s prior to TMS pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siann\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 0.24.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# yasa sleep stageing (not very accurate probably because data is artificially cut)\n",
    "sleep_stages = yasa.SleepStaging(raw_data, eeg_name = 'C4', eog_name=\"VEOG\", emg_name=\"EMG\") \n",
    "values = sleep_stages.predict() # predict the sleep stages\n",
    "from yasa import Hypnogram\n",
    "hyp = Hypnogram(values, n_stages=5) # create yasa Hypnogram object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siann\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\yasa\\hypno.py:483: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return self.hypno.replace(self.mapping).astype(np.int16)\n",
      "c:\\Users\\siann\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\yasa\\hypno.py:483: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  return self.hypno.replace(self.mapping).astype(np.int16)\n",
      "04-Apr-24 15:50:57 | WARNING | Hypnogram is SHORTER than data by 9.00 seconds. Padding hypnogram with last value to match data.size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    1.6s\n",
      "04-Apr-24 15:51:08 | WARNING | No SW were found in channel CHAN030.\n",
      "04-Apr-24 15:51:08 | WARNING | No SW were found in channel CHAN031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Start   NegPeak  MidCrossing   PosPeak       End  Duration  \\\n",
      "0    315.7762  316.6014     317.1204  317.8008  318.0658    2.2896   \n",
      "1    318.0658  318.2980     318.5648  319.2064  319.4512    1.3854   \n",
      "2    319.4512  319.7290     320.6218  320.8006  321.0880    1.6368   \n",
      "3    354.0116  354.3332     354.7268  355.2946  355.6060    1.5944   \n",
      "4    355.6060  355.7954     356.0880  356.7318  356.9660    1.3600   \n",
      "..        ...       ...          ...       ...       ...       ...   \n",
      "200  340.3000  340.6172     341.0592  341.2300  341.3954    1.0954   \n",
      "201  367.3694  367.6516     367.9054  368.1066  368.3434    0.9740   \n",
      "202  304.9840  305.1900     305.4386  305.5430  305.6568    0.6728   \n",
      "203  308.8194  309.1764     309.8338  310.0104  310.2268    1.4074   \n",
      "204  349.3692  349.6500     349.9096  350.1280  350.8718    1.5026   \n",
      "\n",
      "     ValNegPeak  ValPosPeak         PTP       Slope  Frequency  Stage  \\\n",
      "0    -70.579018  118.978570  189.557588  365.236200   0.436758      3   \n",
      "1   -110.278770  105.856888  216.135657  810.103664   0.721813      3   \n",
      "2   -138.897776   12.587033  151.484809  169.673845   0.610948      3   \n",
      "3   -185.883871  121.151415  307.035285  780.069323   0.627195      3   \n",
      "4    -51.742339   36.105253   87.847592  300.231006   0.735294      3   \n",
      "..          ...         ...         ...         ...        ...    ...   \n",
      "200 -115.799036   18.516723  134.315759  303.881809   0.912909      3   \n",
      "201  -48.500204   32.210049   80.710253  318.007301   1.026694      3   \n",
      "202 -104.190990   23.334696  127.525687  512.975410   1.486326      3   \n",
      "203 -103.851189   13.946713  117.797902  179.187560   0.710530      3   \n",
      "204  -77.488578   52.696563  130.185141  501.483594   0.665513      3   \n",
      "\n",
      "     Channel  IdxChannel  \n",
      "0    CHAN000           0  \n",
      "1    CHAN000           0  \n",
      "2    CHAN000           0  \n",
      "3    CHAN000           0  \n",
      "4    CHAN000           0  \n",
      "..       ...         ...  \n",
      "200  CHAN068          68  \n",
      "201  CHAN068          68  \n",
      "202  CHAN069          69  \n",
      "203  CHAN069          69  \n",
      "204  CHAN069          69  \n",
      "\n",
      "[205 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# detect slow waves\n",
    "hyp_up = hyp.upsample_to_data(sf = 1/30, data = raw_data)\n",
    "data = raw_data.get_data() # get data from all channels\n",
    "slow_oscillations = yasa.sw_detect(data, include=(1,2,3,4), hypno=hyp_up, sf = 5000, freq_sw=(0.3, 1.5),\n",
    "                                      dur_neg=(0.3, 1.5), dur_pos=(0.1, 1), amp_neg=(40, 200), amp_pos=(10, 150),\n",
    "                                      amp_ptp=(75, 350), coupling=False) # still change detection parameters?\n",
    "slow_oscillations_df = slow_oscillations.summary()\n",
    "print(slow_oscillations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({False: 76, True: 6})\n",
      "    Stimulation Times (sec)  Trial start (sec)  condition  so_occuring  \\\n",
      "0                       2.0                  0          2        False   \n",
      "1                       7.0                  4          5        False   \n",
      "2                      11.0                  9          4        False   \n",
      "3                      16.0                 13          3        False   \n",
      "4                      20.0                 18          1        False   \n",
      "..                      ...                ...        ...          ...   \n",
      "77                    349.0                346          4        False   \n",
      "78                    353.0                351          2        False   \n",
      "79                    358.0                355          5         True   \n",
      "80                    362.0                360          3         True   \n",
      "81                    367.0                364          1        False   \n",
      "\n",
      "    so_occuring_trial_index  so_occuring_trough_time  \n",
      "0                       NaN                      NaN  \n",
      "1                       NaN                      NaN  \n",
      "2                       NaN                      NaN  \n",
      "3                       NaN                      NaN  \n",
      "4                       NaN                      NaN  \n",
      "..                      ...                      ...  \n",
      "77                      NaN                      NaN  \n",
      "78                      NaN                      NaN  \n",
      "79                    153.0                 356.7868  \n",
      "80                     92.0                 361.9048  \n",
      "81                      NaN                      NaN  \n",
      "\n",
      "[82 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# get stimulation times\n",
    "stimulation_times_list = []\n",
    "trial_start_list = []\n",
    "condition_list = []\n",
    "for i in events:\n",
    "        stimulation_time = (i[0] + 2.5*sf) // sf  # get stimulation time in seconds \n",
    "        stimulation_times_list.append(stimulation_time)  \n",
    "        trial_start = (i[0]) // sf  \n",
    "        trial_start_list.append(trial_start)  \n",
    "        condition = i[2]\n",
    "        condition_list.append(condition)\n",
    "\n",
    "# count how many times stimulation occured as the same time as detected SO and store in df\n",
    "stimulation_times_df = pd.DataFrame({'Stimulation Times (sec)': stimulation_times_list, 'Trial start (sec)': trial_start_list, \n",
    "                                    'condition': condition_list})\n",
    "stimulation_times_df['so_occuring'] = False\n",
    "stimulation_times_df['so_occuring_trial_index'] = np.NaN\n",
    "stimulation_times_df['so_occuring_trough_time'] = np.NaN\n",
    "for index, entry in slow_oscillations_df.iterrows():\n",
    "    start_time = entry['Start']\n",
    "    end_time = entry['End']\n",
    "    for i, row  in stimulation_times_df.iterrows():\n",
    "        if (start_time <= row['Stimulation Times (sec)'] <= end_time): \n",
    "            stimulation_times_df.at[i, 'so_occuring'] = True\n",
    "            stimulation_times_df.at[i, 'so_occuring_trial_index'] = index\n",
    "            stimulation_times_df.at[i, 'so_occuring_trough_time'] = entry['NegPeak']\n",
    "            stimulation_times_df.at[i, 'so_occuring_trough_time'] = entry['NegPeak']\n",
    "            stimulation_times_df.at[i, 'so_occuring_trough_time'] = entry['NegPeak']\n",
    "number_co_occurences = Counter(stimulation_times_df['so_occuring'])\n",
    "print(number_co_occurences)\n",
    "print(stimulation_times_df)\n",
    "# write to csv\n",
    "# stimulation_times_df.to_csv(path_or_buf=(datapath + '\\\\' + subjects ), sep=',', na_rep='', float_format=None, columns=None, header=True, \n",
    "                 # index=True, index_label=None, mode = 'w')\n",
    "stimulation_times_df.to_csv('stimulation_times_so_cooccur.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
